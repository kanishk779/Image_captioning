{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "# !pip install bcolz\n",
    "import bcolz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from functools import reduce\n",
    "# from tqdm.notebook import tqdm\n",
    "import string\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import urllib\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import nltk\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as datasets \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "spacy_eng = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_img(file):\n",
    "    img_id=[]\n",
    "    for img in glob.glob(file+'*.jpg'):\n",
    "        temp=os.path.basename(img)\n",
    "        img_id.append(temp)\n",
    "        \n",
    "    return img_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the data from tar files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use glove\n",
    "glove_path = '/scratch/varunc/glove/'\n",
    "glove_zip = glove_path+'glove.6B.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_len(fname):\n",
    "    with open(fname) as f:\n",
    "        for i, l in enumerate(f):\n",
    "            pass\n",
    "    return i + 1\n",
    "\n",
    "\n",
    "# Need not run this again and again\n",
    "\n",
    "\n",
    "words = []\n",
    "idx = 0\n",
    "word2idx = {}\n",
    "vectors = bcolz.carray(np.zeros(1), rootdir=glove_path+'6B.300.dat', mode='w')\n",
    "\n",
    "with open(glove_path+'glove.6B.300d.txt', 'rb') as f:\n",
    "    pbar = tqdm(total=file_len(glove_path+'glove.6B.300d.txt'))\n",
    "    for l in f:\n",
    "        line = l.decode().split()\n",
    "        word = line[0]\n",
    "        words.append(word)\n",
    "        word2idx[word] = idx\n",
    "        idx += 1\n",
    "        vect = np.array(line[1:]).astype(np.float)\n",
    "        vectors.append(vect)\n",
    "        pbar.update(1)\n",
    "    \n",
    "vectors = bcolz.carray(vectors[1:].reshape((400000, 300)), rootdir=glove_path+'6B.300.dat', mode='w')\n",
    "vectors.flush()\n",
    "pickle.dump(words, open(glove_path+'6B.300_words.pkl', 'wb'))\n",
    "pickle.dump(word2idx, open(glove_path+'6B.300_idx.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = bcolz.open(glove_path+'6B.300.dat')[:]\n",
    "words = pickle.load(open(glove_path+'6B.300_words.pkl', 'rb'))\n",
    "word2idx = pickle.load(open(glove_path+'6B.300_idx.pkl', 'rb'))\n",
    "\n",
    "glove = {w: vectors[word2idx[w]] for w in words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_imgid=read_img(flickr_path+\"Flickr-8K/Flicker8k_Dataset/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, freq_thresh):\n",
    "        self.freq_thresh = freq_thresh\n",
    "        self.itos = {0:\"<PAD>\", 1:\"<SOS>\", 2:\"<EOS>\", 3:\"<UNK>\"}\n",
    "        self.stoi = {\"<PAD>\":0, \"<SOS>\":1, \"<EOS>\":2, \"<UNK>\":3}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenizer(text):\n",
    "        return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n",
    "\n",
    "    def build_vocab(self, sentence_list):\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        frequencies = {}\n",
    "        idx = 4\n",
    "        for sentence in sentence_list:\n",
    "            for word in self.tokenizer(sentence):\n",
    "                lw = lemmatizer.lemmatize(word)\n",
    "                if word not in frequencies:\n",
    "                    frequencies[word] = 1\n",
    "                else:\n",
    "                    frequencies[word] += 1\n",
    "                if frequencies[word] == self.freq_thresh:\n",
    "                    self.stoi[word] = idx\n",
    "                    self.itos[idx] = word\n",
    "                    idx += 1\n",
    "                    \n",
    "                if lw not in frequencies:\n",
    "                    frequencies[lw] = 1\n",
    "                else:\n",
    "                    frequencies[lw] += 1\n",
    "                if frequencies[lw] == self.freq_thresh:\n",
    "                    self.stoi[lw] = idx\n",
    "                    self.itos[idx] = lw\n",
    "                    idx += 1\n",
    "    \n",
    "    def numericalize(self, text):\n",
    "        tok_text = self.tokenizer(text)\n",
    "        vec = [self.stoi[word] if word in self.stoi else self.stoi[\"<UNK>\"] for word in tok_text]\n",
    "        return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for test and train we need to create new text files of the format (Flickr8k.token.txt)\n",
    "class FlickrDataset(Dataset):\n",
    "    def __init__(self, root_dir, caption_file, lemma_file, img_file, freq_thresh, transform=None,vocab = None, lemma_vocab=None):\n",
    "        self.freq_thresh = freq_thresh\n",
    "        self.transform = transform\n",
    "        self.root_dir = root_dir\n",
    "        self.img_file = img_file\n",
    "        self.caption_file = caption_file\n",
    "        self.lemma_file = lemma_file\n",
    "        \n",
    "        self.caption_dict = self.imgId_caption_dict()\n",
    "        self.lemma_dict = self.imgId_lemma_dict()\n",
    "        \n",
    "        self.imgs , self.captions, self.all_captions = self.load_img_caption()\n",
    "        if vocab is None:\n",
    "            self.vocab = Vocabulary(self.freq_thresh)\n",
    "            self.vocab.build_vocab(self.captions)\n",
    "        else:\n",
    "            self.vocab = vocab\n",
    "            \n",
    "        self.imgs_lemma , self.lemmas = self.load_img_lemma()\n",
    "        if lemma_vocab is None:\n",
    "            self.lemma_vocab = Vocabulary(self.freq_thresh)\n",
    "            self.lemma_vocab.build_vocab(self.lemmas)\n",
    "        else:\n",
    "            self.lemma_vocab = lemma_vocab\n",
    "\n",
    "    def imgId_caption_dict(self):\n",
    "        caption_dict = {}\n",
    "        with open(self.caption_file, 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                line = line.strip('\\n')\n",
    "                temp = line.split()\n",
    "                img_name, _ = temp[0].split('#')  # first word will be img_id\n",
    "                description = \" \".join([ word for word in temp[1:]]) # get back the description\n",
    "                if img_name not in caption_dict:\n",
    "                    caption_dict[img_name] = [description]\n",
    "                else:\n",
    "                    caption_dict[img_name].append(description)\n",
    "        return caption_dict\n",
    "\n",
    "    def imgId_lemma_dict(self):\n",
    "        stop_word=list(set(stopwords.words('english')))\n",
    "        lemma_dict = {}\n",
    "        with open(self.lemma_file, 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                line = line.strip('\\n')\n",
    "                temp = line.split()\n",
    "                img_name, _ = temp[0].split('#')  # first word will be img_id\n",
    "                description = \" \".join([ word.lower() for word in temp[1:] if word.lower() not in stop_word]) # get back the description\n",
    "                description = description.translate(str.maketrans('', '', string.punctuation)).strip()\n",
    "                if img_name not in lemma_dict:\n",
    "                    lemma_dict[img_name] = [description]\n",
    "                else:\n",
    "                    lemma_dict[img_name].append(description)\n",
    "        return lemma_dict\n",
    "\n",
    "    def load_img_caption(self):\n",
    "        imgs = []\n",
    "        captions = []\n",
    "        all_captions = []\n",
    "        with open(self.img_file, 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                line = line.strip('\\n')\n",
    "                for caption in self.caption_dict[line]:\n",
    "                    imgs.append(line)\n",
    "                    captions.append(caption)\n",
    "                    all_captions.append(self.caption_dict[line])\n",
    "        return imgs, captions, all_captions\n",
    "\n",
    "    def load_img_lemma(self):\n",
    "        imgs = []\n",
    "        captions = []\n",
    "        with open(self.img_file, 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                line = line.strip('\\n')\n",
    "                for caption in self.lemma_dict[line]:\n",
    "                    imgs.append(line)\n",
    "                    captions.append(caption)\n",
    "        return imgs, captions\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        caption = self.captions[index]\n",
    "        all_caption = self.all_captions[index]\n",
    "        img_id = self.imgs[index]\n",
    "        img = Image.open(os.path.join(self.root_dir, img_id)).convert(\"RGB\")\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        numericalized_caption = [self.vocab.stoi[\"<SOS>\"]]\n",
    "        numericalized_caption += self.vocab.numericalize(caption)\n",
    "        numericalized_caption += [self.vocab.stoi[\"<EOS>\"]]\n",
    "        \n",
    "        all_num_captions = []\n",
    "        for caption_i in all_caption:\n",
    "            num_cap = [self.vocab.stoi[\"<SOS>\"]]\n",
    "            num_cap += self.vocab.numericalize(caption_i)\n",
    "            num_cap += [self.vocab.stoi[\"<EOS>\"]]\n",
    "            all_num_captions += num_cap\n",
    "\n",
    "        return img, torch.tensor(numericalized_caption), torch.tensor(all_num_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also define simply a function instead of a class\n",
    "class MyCollate:\n",
    "    def __init__(self, pad_idx):\n",
    "        self.pad_idx = pad_idx\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        imgs = [item[0].unsqueeze(0) for item in batch]\n",
    "        imgs = torch.cat(imgs, dim=0) # batch * imgs_size * 3 (RGB)\n",
    "        captions = [item[1] for item in batch] \n",
    "        captions = pad_sequence(captions, batch_first=True, padding_value=self.pad_idx)\n",
    "        all_captions = [item[2] for item in batch]\n",
    "        all_captions = pad_sequence(all_captions, batch_first=True, padding_value=self.pad_idx)        \n",
    "        return imgs, captions, all_captions # return the batched images and captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(root_dir, caption_file, lemma_file, img_file, transform, vocab=None, lemma_vocab=None, batch=32, num_worker=2, shuffle=True, pin_memory=True):\n",
    "    dataset = FlickrDataset(root_dir=root_dir, caption_file=caption_file, lemma_file=lemma_file, img_file=img_file, freq_thresh=2, transform=transform,vocab=vocab, lemma_vocab=lemma_vocab)\n",
    "    pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n",
    "    loader = DataLoader(dataset=dataset,\n",
    "                        batch_size=batch,\n",
    "                        shuffle=shuffle,\n",
    "                        collate_fn=MyCollate(pad_idx),\n",
    "                        pin_memory=pin_memory,\n",
    "                        num_workers=num_worker)\n",
    "    return dataset,loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = transforms.Compose([\n",
    "                            transforms.Resize((224, 224)),\n",
    "                            transforms.ToTensor(),\n",
    "])\n",
    "dataset, dataloader = get_loader(root_dir=flickr_path+\"Flickr-8K/Flicker8k_Dataset\",\n",
    "                        img_file=flickr_path+\"Flickr-8K/Flickr_8k.trainImages.txt\",\n",
    "                        caption_file=flickr_path+\"Flickr-8K/Flickr8k.token.txt\",\n",
    "                        lemma_file=flickr_path+\"Flickr-8K/Flickr8k.lemma.token.txt\",\n",
    "                        transform=trans, batch=512,num_worker=30,\n",
    "                        shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset, test_dataloader = get_loader(root_dir=flickr_path+\"Flickr-8K/Flicker8k_Dataset\",\n",
    "                        img_file=flickr_path+\"Flickr-8K/Flickr_8k.testImages.txt\",\n",
    "                        caption_file=flickr_path+\"Flickr-8K/Flickr8k.token.txt\",\n",
    "                        lemma_file=flickr_path+\"Flickr-8K/Flickr8k.lemma.token.txt\",\n",
    "                        transform=trans,\n",
    "                        vocab=dataset.vocab,\n",
    "                        lemma_vocab=dataset.lemma_vocab,\n",
    "                        batch=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GoogleNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        googlenet = torch.hub.load('pytorch/vision:v0.9.0', 'googlenet', pretrained=True)\n",
    "        for param in googlenet.parameters():\n",
    "            param.requires_grad_(False)\n",
    "\n",
    "        modules = list(googlenet.children())[:-2]\n",
    "        self.googlenet = nn.Sequential(*modules)\n",
    "    def forward(self, X):\n",
    "        x = self.googlenet(X)\n",
    "        x = x.view(x.shape[0],-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.DataParallel(EncoderCNN())\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def out_vect(filename):\n",
    "    input_image = Image.open(filename)\n",
    "    input_tensor = trans(input_image)\n",
    "    input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
    "    input_batch = input_batch.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_batch)\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def out_vect_tensor(input_tensor):\n",
    "    input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
    "    input_batch = input_batch.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_batch)\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(file,directory):\n",
    "    img_vector=[]\n",
    "    imgs_list = []\n",
    "    with open(file,'r') as f:\n",
    "        for line in f.readlines():\n",
    "            x=out_vect(os.path.join(directory,line.strip()))\n",
    "            img_vector.append(x)\n",
    "            imgs_list.append(line.strip())\n",
    "    return imgs_vector, imgs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_vector, imgs_list=get_vector(flickr_path+'Flickr-8K/Flickr_8k.trainImages.txt',\n",
    "                                  flickr_path+\"Flickr-8K/Flicker8k_Dataset/\")\n",
    "imgs_list = np.array(imgs_list)\n",
    "imgs_tensor = torch.cat(imgs_vector,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(data, test, num=5):\n",
    "    dist = torch.norm(data - test, dim=1, p=2)\n",
    "    knn = dist.topk(num, largest=False)\n",
    "    \n",
    "    return knn.indices.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_ll(img_list):\n",
    "    return [ dataset.lemma_dict[x] for x in img_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attr_encoded(vect, nums=10, train=True):\n",
    "    if train:\n",
    "        words_pref_ll = get_words_ll(imgs_list[knn(attr_tensor,vect,nums+1)[1:]])\n",
    "    else:\n",
    "        words_pref_ll = get_words_ll(imgs_list[knn(attr_tensor,vect,nums)])\n",
    "    all_words = ' '.join(reduce(lambda x,y: x+y,words_pref_ll)).split()\n",
    "    all_words = [ x for x in all_words if x in dataset.vocab.stoi]\n",
    "    top_words = [ x for x,y in Counter(all_words).most_common(nums)]\n",
    "    return top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attr_tensor(img_tensor,train=True):\n",
    "    model.to(device)\n",
    "    img_tensor = img_tensor.to(device)\n",
    "    with torch.no_grad():\n",
    "        vect = model(img_tensor)\n",
    "    return get_attr_encoded(vect, train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
