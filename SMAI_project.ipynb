{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "# !pip install bcolz\n",
    "import bcolz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from functools import reduce\n",
    "# from tqdm.notebook import tqdm\n",
    "import string\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import urllib\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import nltk\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as datasets \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "spacy_eng = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_img(file):\n",
    "    img_id=[]\n",
    "    for img in glob.glob(file+'*.jpg'):\n",
    "        temp=os.path.basename(img)\n",
    "        img_id.append(temp)\n",
    "        \n",
    "    return img_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the data from tar files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use glove\n",
    "glove_path = '/scratch/varunc/glove/'\n",
    "glove_zip = glove_path+'glove.6B.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_len(fname):\n",
    "    with open(fname) as f:\n",
    "        for i, l in enumerate(f):\n",
    "            pass\n",
    "    return i + 1\n",
    "\n",
    "\n",
    "# Need not run this again and again\n",
    "\n",
    "\n",
    "words = []\n",
    "idx = 0\n",
    "word2idx = {}\n",
    "vectors = bcolz.carray(np.zeros(1), rootdir=glove_path+'6B.300.dat', mode='w')\n",
    "\n",
    "with open(glove_path+'glove.6B.300d.txt', 'rb') as f:\n",
    "    pbar = tqdm(total=file_len(glove_path+'glove.6B.300d.txt'))\n",
    "    for l in f:\n",
    "        line = l.decode().split()\n",
    "        word = line[0]\n",
    "        words.append(word)\n",
    "        word2idx[word] = idx\n",
    "        idx += 1\n",
    "        vect = np.array(line[1:]).astype(np.float)\n",
    "        vectors.append(vect)\n",
    "        pbar.update(1)\n",
    "    \n",
    "vectors = bcolz.carray(vectors[1:].reshape((400000, 300)), rootdir=glove_path+'6B.300.dat', mode='w')\n",
    "vectors.flush()\n",
    "pickle.dump(words, open(glove_path+'6B.300_words.pkl', 'wb'))\n",
    "pickle.dump(word2idx, open(glove_path+'6B.300_idx.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = bcolz.open(glove_path+'6B.300.dat')[:]\n",
    "words = pickle.load(open(glove_path+'6B.300_words.pkl', 'rb'))\n",
    "word2idx = pickle.load(open(glove_path+'6B.300_idx.pkl', 'rb'))\n",
    "\n",
    "glove = {w: vectors[word2idx[w]] for w in words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_imgid=read_img(flickr_path+\"Flickr-8K/Flicker8k_Dataset/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, freq_thresh):\n",
    "        self.freq_thresh = freq_thresh\n",
    "        self.itos = {0:\"<PAD>\", 1:\"<SOS>\", 2:\"<EOS>\", 3:\"<UNK>\"}\n",
    "        self.stoi = {\"<PAD>\":0, \"<SOS>\":1, \"<EOS>\":2, \"<UNK>\":3}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenizer(text):\n",
    "        return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n",
    "\n",
    "    def build_vocab(self, sentence_list):\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        frequencies = {}\n",
    "        idx = 4\n",
    "        for sentence in sentence_list:\n",
    "            for word in self.tokenizer(sentence):\n",
    "                lw = lemmatizer.lemmatize(word)\n",
    "                if word not in frequencies:\n",
    "                    frequencies[word] = 1\n",
    "                else:\n",
    "                    frequencies[word] += 1\n",
    "                if frequencies[word] == self.freq_thresh:\n",
    "                    self.stoi[word] = idx\n",
    "                    self.itos[idx] = word\n",
    "                    idx += 1\n",
    "                    \n",
    "                if lw not in frequencies:\n",
    "                    frequencies[lw] = 1\n",
    "                else:\n",
    "                    frequencies[lw] += 1\n",
    "                if frequencies[lw] == self.freq_thresh:\n",
    "                    self.stoi[lw] = idx\n",
    "                    self.itos[idx] = lw\n",
    "                    idx += 1\n",
    "    \n",
    "    def numericalize(self, text):\n",
    "        tok_text = self.tokenizer(text)\n",
    "        vec = [self.stoi[word] if word in self.stoi else self.stoi[\"<UNK>\"] for word in tok_text]\n",
    "        return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for test and train we need to create new text files of the format (Flickr8k.token.txt)\n",
    "class FlickrDataset(Dataset):\n",
    "    def __init__(self, root_dir, caption_file, lemma_file, img_file, freq_thresh, transform=None,vocab = None, lemma_vocab=None):\n",
    "        self.freq_thresh = freq_thresh\n",
    "        self.transform = transform\n",
    "        self.root_dir = root_dir\n",
    "        self.img_file = img_file\n",
    "        self.caption_file = caption_file\n",
    "        self.lemma_file = lemma_file\n",
    "        \n",
    "        self.caption_dict = self.imgId_caption_dict()\n",
    "        self.lemma_dict = self.imgId_lemma_dict()\n",
    "        \n",
    "        self.imgs , self.captions, self.all_captions = self.load_img_caption()\n",
    "        if vocab is None:\n",
    "            self.vocab = Vocabulary(self.freq_thresh)\n",
    "            self.vocab.build_vocab(self.captions)\n",
    "        else:\n",
    "            self.vocab = vocab\n",
    "            \n",
    "        self.imgs_lemma , self.lemmas = self.load_img_lemma()\n",
    "        if lemma_vocab is None:\n",
    "            self.lemma_vocab = Vocabulary(self.freq_thresh)\n",
    "            self.lemma_vocab.build_vocab(self.lemmas)\n",
    "        else:\n",
    "            self.lemma_vocab = lemma_vocab\n",
    "\n",
    "    def imgId_caption_dict(self):\n",
    "        caption_dict = {}\n",
    "        with open(self.caption_file, 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                line = line.strip('\\n')\n",
    "                temp = line.split()\n",
    "                img_name, _ = temp[0].split('#')  # first word will be img_id\n",
    "                description = \" \".join([ word for word in temp[1:]]) # get back the description\n",
    "                if img_name not in caption_dict:\n",
    "                    caption_dict[img_name] = [description]\n",
    "                else:\n",
    "                    caption_dict[img_name].append(description)\n",
    "        return caption_dict\n",
    "\n",
    "    def imgId_lemma_dict(self):\n",
    "        stop_word=list(set(stopwords.words('english')))\n",
    "        lemma_dict = {}\n",
    "        with open(self.lemma_file, 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                line = line.strip('\\n')\n",
    "                temp = line.split()\n",
    "                img_name, _ = temp[0].split('#')  # first word will be img_id\n",
    "                description = \" \".join([ word.lower() for word in temp[1:] if word.lower() not in stop_word]) # get back the description\n",
    "                description = description.translate(str.maketrans('', '', string.punctuation)).strip()\n",
    "                if img_name not in lemma_dict:\n",
    "                    lemma_dict[img_name] = [description]\n",
    "                else:\n",
    "                    lemma_dict[img_name].append(description)\n",
    "        return lemma_dict\n",
    "\n",
    "    def load_img_caption(self):\n",
    "        imgs = []\n",
    "        captions = []\n",
    "        all_captions = []\n",
    "        with open(self.img_file, 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                line = line.strip('\\n')\n",
    "                for caption in self.caption_dict[line]:\n",
    "                    imgs.append(line)\n",
    "                    captions.append(caption)\n",
    "                    all_captions.append(self.caption_dict[line])\n",
    "        return imgs, captions, all_captions\n",
    "\n",
    "    def load_img_lemma(self):\n",
    "        imgs = []\n",
    "        captions = []\n",
    "        with open(self.img_file, 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                line = line.strip('\\n')\n",
    "                for caption in self.lemma_dict[line]:\n",
    "                    imgs.append(line)\n",
    "                    captions.append(caption)\n",
    "        return imgs, captions\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        caption = self.captions[index]\n",
    "        all_caption = self.all_captions[index]\n",
    "        img_id = self.imgs[index]\n",
    "        img = Image.open(os.path.join(self.root_dir, img_id)).convert(\"RGB\")\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        numericalized_caption = [self.vocab.stoi[\"<SOS>\"]]\n",
    "        numericalized_caption += self.vocab.numericalize(caption)\n",
    "        numericalized_caption += [self.vocab.stoi[\"<EOS>\"]]\n",
    "        \n",
    "        all_num_captions = []\n",
    "        for caption_i in all_caption:\n",
    "            num_cap = [self.vocab.stoi[\"<SOS>\"]]\n",
    "            num_cap += self.vocab.numericalize(caption_i)\n",
    "            num_cap += [self.vocab.stoi[\"<EOS>\"]]\n",
    "            all_num_captions += num_cap\n",
    "\n",
    "        return img, torch.tensor(numericalized_caption), torch.tensor(all_num_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also define simply a function instead of a class\n",
    "class MyCollate:\n",
    "    def __init__(self, pad_idx):\n",
    "        self.pad_idx = pad_idx\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        imgs = [item[0].unsqueeze(0) for item in batch]\n",
    "        imgs = torch.cat(imgs, dim=0) # batch * imgs_size * 3 (RGB)\n",
    "        captions = [item[1] for item in batch] \n",
    "        captions = pad_sequence(captions, batch_first=True, padding_value=self.pad_idx)\n",
    "        all_captions = [item[2] for item in batch]\n",
    "        all_captions = pad_sequence(all_captions, batch_first=True, padding_value=self.pad_idx)        \n",
    "        return imgs, captions, all_captions # return the batched images and captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(root_dir, caption_file, lemma_file, img_file, transform, vocab=None, lemma_vocab=None, batch=32, num_worker=2, shuffle=True, pin_memory=True):\n",
    "    dataset = FlickrDataset(root_dir=root_dir, caption_file=caption_file, lemma_file=lemma_file, img_file=img_file, freq_thresh=2, transform=transform,vocab=vocab, lemma_vocab=lemma_vocab)\n",
    "    pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n",
    "    loader = DataLoader(dataset=dataset,\n",
    "                        batch_size=batch,\n",
    "                        shuffle=shuffle,\n",
    "                        collate_fn=MyCollate(pad_idx),\n",
    "                        pin_memory=pin_memory,\n",
    "                        num_workers=num_worker)\n",
    "    return dataset,loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = transforms.Compose([\n",
    "                            transforms.Resize((224, 224)),\n",
    "                            transforms.ToTensor(),\n",
    "])\n",
    "dataset, dataloader = get_loader(root_dir=flickr_path+\"Flickr-8K/Flicker8k_Dataset\",\n",
    "                        img_file=flickr_path+\"Flickr-8K/Flickr_8k.trainImages.txt\",\n",
    "                        caption_file=flickr_path+\"Flickr-8K/Flickr8k.token.txt\",\n",
    "                        lemma_file=flickr_path+\"Flickr-8K/Flickr8k.lemma.token.txt\",\n",
    "                        transform=trans, batch=512,num_worker=30,\n",
    "                        shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset, test_dataloader = get_loader(root_dir=flickr_path+\"Flickr-8K/Flicker8k_Dataset\",\n",
    "                        img_file=flickr_path+\"Flickr-8K/Flickr_8k.testImages.txt\",\n",
    "                        caption_file=flickr_path+\"Flickr-8K/Flickr8k.token.txt\",\n",
    "                        lemma_file=flickr_path+\"Flickr-8K/Flickr8k.lemma.token.txt\",\n",
    "                        transform=trans,\n",
    "                        vocab=dataset.vocab,\n",
    "                        lemma_vocab=dataset.lemma_vocab,\n",
    "                        batch=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GoogleNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        googlenet = torch.hub.load('pytorch/vision:v0.9.0', 'googlenet', pretrained=True)\n",
    "        for param in googlenet.parameters():\n",
    "            param.requires_grad_(False)\n",
    "\n",
    "        modules = list(googlenet.children())[:-2]\n",
    "        self.googlenet = nn.Sequential(*modules)\n",
    "    def forward(self, X):\n",
    "        x = self.googlenet(X)\n",
    "        x = x.view(x.shape[0],-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.DataParallel(EncoderCNN())\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def out_vect(filename):\n",
    "    input_image = Image.open(filename)\n",
    "    input_tensor = trans(input_image)\n",
    "    input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
    "    input_batch = input_batch.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_batch)\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def out_vect_tensor(input_tensor):\n",
    "    input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
    "    input_batch = input_batch.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_batch)\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(file,directory):\n",
    "    img_vector=[]\n",
    "    imgs_list = []\n",
    "    with open(file,'r') as f:\n",
    "        for line in f.readlines():\n",
    "            x=out_vect(os.path.join(directory,line.strip()))\n",
    "            img_vector.append(x)\n",
    "            imgs_list.append(line.strip())\n",
    "    return imgs_vector, imgs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_vector, imgs_list=get_vector(flickr_path+'Flickr-8K/Flickr_8k.trainImages.txt',\n",
    "                                  flickr_path+\"Flickr-8K/Flicker8k_Dataset/\")\n",
    "imgs_list = np.array(imgs_list)\n",
    "imgs_tensor = torch.cat(imgs_vector,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(data, test, num=5):\n",
    "    dist = torch.norm(data - test, dim=1, p=2)\n",
    "    knn = dist.topk(num, largest=False)\n",
    "    \n",
    "    return knn.indices.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_ll(img_list):\n",
    "    return [ dataset.lemma_dict[x] for x in img_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attr_encoded(vect, nums=10, train=True):\n",
    "    if train:\n",
    "        words_pref_ll = get_words_ll(imgs_list[knn(attr_tensor,vect,nums+1)[1:]])\n",
    "    else:\n",
    "        words_pref_ll = get_words_ll(imgs_list[knn(attr_tensor,vect,nums)])\n",
    "    all_words = ' '.join(reduce(lambda x,y: x+y,words_pref_ll)).split()\n",
    "    all_words = [ x for x in all_words if x in dataset.vocab.stoi]\n",
    "    top_words = [ x for x,y in Counter(all_words).most_common(nums)]\n",
    "    return top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attr_tensor(img_tensor,train=True):\n",
    "    model.to(device)\n",
    "    img_tensor = img_tensor.to(device)\n",
    "    with torch.no_grad():\n",
    "        vect = model(img_tensor)\n",
    "    return get_attr_encoded(vect, train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Layer using GLoVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_vocab = dataset.vocab.itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_len = len(target_vocab)  # target vocabulary \n",
    "weights_matrix = np.zeros((matrix_len, 300))\n",
    "words_found = 0\n",
    "\n",
    "for i, word in enumerate(target_vocab):\n",
    "    try: \n",
    "        weights_matrix[i] = glove[word]\n",
    "        words_found += 1\n",
    "    except KeyError:\n",
    "        weights_matrix[i] = np.random.normal(scale=0.6, size=(300, ))\n",
    "weights_matrix = torch.tensor(weights_matrix,requires_grad = False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emb_layer(weights_matrix, non_trainable=False):\n",
    "    num_embeddings, embedding_dim = weights_matrix.size()\n",
    "    emb_layer = nn.Embedding(num_embeddings, embedding_dim).to(device)\n",
    "    emb_layer.load_state_dict({'weight': weights_matrix})\n",
    "    if non_trainable:\n",
    "        emb_layer.weight.requires_grad = False\n",
    "\n",
    "    return emb_layer, num_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for Decoder RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    takes as input img vector and captions\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input_size = 512  # m = input_size in paper\n",
    "        self.hidden_size = 512 # n = hidden_size in paper\n",
    "        self.word_size = 300\n",
    "        self.image_size = 1024\n",
    "        self.lstm = nn.LSTMCell(self.input_size, self.hidden_size) # using single cell rather than lstm\n",
    "        self.U = nn.Linear(self.word_size, self.word_size)\n",
    "        self.V = nn.Linear(self.hidden_size, self.word_size)\n",
    "        \n",
    "        self.W_x_Y = nn.Linear(self.word_size, self.input_size)\n",
    "        self.W_Y_h = nn.Linear(self.hidden_size, self.word_size)\n",
    "        self.image_mapping = nn.Linear(self.image_size, self.input_size)\n",
    "        \n",
    "        self.W_x_A = nn.Linear(self.word_size, self.word_size)\n",
    "        self.W_Y_A = nn.Linear(self.word_size, self.word_size)\n",
    "        \n",
    "        # use the pretrained glove embedding\n",
    "        self.vocab, self.num_embedding = create_emb_layer(weights_matrix, True)  # num_embeddings x 300\n",
    "        self.soft = nn.Softmax(dim=1)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.ReLU = nn.ReLU(inplace = True)\n",
    "        self.vocab = self.vocab.to(device)\n",
    "\n",
    "    def input_attention(self, previous_word, attributes):\n",
    "        \"\"\" \n",
    "        Apply input attention at each step.\n",
    "        attributes -> matrix where columns store the word embedding, batch x no_of_attr x 300\n",
    "        previous word -> batch x 300\n",
    "        \"\"\"\n",
    "        # for each word in the vocabulary find the score\n",
    "        score = torch.bmm(self.U(previous_word).unsqueeze(1), attributes.transpose(1,2))\n",
    "        score = score.squeeze(1) # batch x no_of_attributes\n",
    "        score = self.soft(score)  # batch x no_of_attributes\n",
    "        weighted_y = torch.bmm(score.unsqueeze(1), attributes).squeeze(1)  # batch x 300\n",
    "        \n",
    "        scaled_y = self.W_x_A(weighted_y)\n",
    "        final_y = scaled_y + previous_word # batch x 300\n",
    "        x = self.W_x_Y(final_y)  # batch x 512\n",
    "        return x, score\n",
    "        \n",
    "    def output_attention(self, hidden_state, attributes):\n",
    "        \"\"\"\n",
    "        Apply output attention at each step\n",
    "        hidden_state = current hidden state of LSTMCell, used for predicting current output (batch x 512)\n",
    "        attributes -> matrix where columns store the word embedding, 300 x no_of_attr\n",
    "        \"\"\"\n",
    "        score = torch.bmm(self.V(hidden_state).unsqueeze(1), self.tanh(attributes).transpose(1,2)) # batch x 1 x no_of_attributes\n",
    "        score = self.soft(score)  # batch x no_of_attributes\n",
    "        weighted_y = torch.bmm(score, self.tanh(attributes)) # batch x 1 x 300\n",
    "        weighted_y = weighted_y.squeeze(1) # batch x 300\n",
    "        \n",
    "        y_to_h = self.W_Y_A(weighted_y)\n",
    "        hidden = self.W_Y_h(hidden_state)  # batch x 300\n",
    "        hidden = hidden + y_to_h  # batch x 300\n",
    "        logits = torch.mm(hidden, self.vocab.weight.transpose(0,1)) # batch x vocab_size\n",
    "        score = score.squeeze(1) # batch x no_of_attr\n",
    "        return logits, score\n",
    "\n",
    "    def prepare_attributes(self, attr):\n",
    "        no_of_attr = attr.shape[1] # batch x no_of_attr\n",
    "        batch = attr.shape[0] \n",
    "        attributes = torch.zeros(batch, no_of_attr, 300).to(device)  # batch x no_of_attr x 300\n",
    "        for i in range(batch):\n",
    "            for j in range(no_of_attr):\n",
    "                attributes[i, j, :] = self.vocab(attr[i, j])\n",
    "        return attributes\n",
    "\n",
    "    def next_word(self, logits):\n",
    "        \"\"\"\n",
    "        Samples from the top 3 words randomly and returns a word\n",
    "        \"\"\"\n",
    "        probability = torch.softmax(logits, dim=1)  # batch x no_of_attr\n",
    "        prob_sort, indices = torch.sort(probability, descending=True)\n",
    "        # generate a array of size -> batch\n",
    "        arr = torch.randint(3, (logits.shape[0],)).to(device)\n",
    "        random_words = torch.zeros(logits.shape[0], 300).to(device)\n",
    "        random_idxs = torch.zeros(logits.shape[0]).to(device)\n",
    "\n",
    "        for i in range(logits.shape[0]):\n",
    "            ind = indices[i, arr[i]]\n",
    "            random_words[i] = self.vocab(ind)\n",
    "            random_idxs[i] =ind\n",
    "        return random_words, random_idxs  # batch x 300\n",
    "\n",
    "    def forward(self, seq_len, image_vectors, attributes, caption_ids=None, train=True):\n",
    "        \"\"\"\n",
    "        image_vectors -> batch x img_size\n",
    "        caption_ids -> batch x seq_len x word_size (make it not None, if you want to do teacher forcing)\n",
    "        attributes -> batch x 5, we have 5 attributes for each of the image (contains indices of words in our dictionary)\n",
    "        it should also contain <PAD>, <SOS>, <EOS> as well. \n",
    "        \"\"\"\n",
    "        image_vectors = self.ReLU(self.image_mapping(image_vectors)) # batch x 512\n",
    "        attributes = self.prepare_attributes(attributes)\n",
    "        batch_size = image_vectors.shape[0]\n",
    "\n",
    "        Hidden_logits = torch.zeros(batch_size, seq_len, self.vocab.weight.shape[0]).to(device)  # batch x seq x vocab_size\n",
    "        Hidden_scores = torch.zeros(batch_size, seq_len, attributes.shape[1]).to(device) # batch x seq x no_of_attr\n",
    "        Input_scores = torch.zeros(batch_size, seq_len, attributes.shape[1]).to(device) # batch x seq x no_of_attr\n",
    "\n",
    "        words = []\n",
    "#         word = self.initial_attention(image_vectors)\n",
    "        \n",
    "        hidden, cell = self.lstm(image_vectors) # batch x 512\n",
    "        logits, hidden_score = self.output_attention(hidden, attributes)\n",
    "        word, word_ind = self.next_word(logits)  # batch x 300\n",
    "#         words.append(word_ind)\n",
    "\n",
    "        for i in range(seq_len):\n",
    "            # prepare the input using input attention\n",
    "            next_input, input_score = self.input_attention(word, attributes)\n",
    "            Input_scores[:, i, :] = input_score\n",
    "            hidden, cell = self.lstm(next_input, (hidden, cell))\n",
    "            logits, hidden_score = self.output_attention(hidden, attributes)\n",
    "            Hidden_scores[:, i, :] = hidden_score\n",
    "            Hidden_logits[:, i, :] = logits\n",
    "            word, word_ind = self.next_word(logits)  # batch x 300\n",
    "            words.append(word_ind)\n",
    "            if not train and word_ind == dataset.vocab.stoi['<EOS>']:\n",
    "                break\n",
    "\n",
    "        return Hidden_logits, Hidden_scores, Input_scores, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "class MyLoss(nn.Module):\n",
    "    \"\"\"Crossentropy + regularization\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.criterion = nn.CrossEntropyLoss(reduction='none').to(device)\n",
    "\n",
    "    def forward(self, output, target, Hidden_scores, Input_scores):\n",
    "        # output -> batch x seq x no_of_attr\n",
    "        # target -> batch x seq\n",
    "        x = self.criterion(output, target).mean(dim=1)  # batch\n",
    "\n",
    "        hidden_reg = self.regularize(Hidden_scores)\n",
    "        input_reg = self.regularize(Input_scores)\n",
    "\n",
    "        ret = x+1e-5*(hidden_reg+input_reg)\n",
    "        ret = ret.mean()\n",
    "        return ret  # equation 10 in paper\n",
    "\n",
    "    def regularize(self, score):\n",
    "        # batch x time x i(attr index)\n",
    "        s1 = torch.sum(score, dim=1)\n",
    "        s1 = s1 * s1\n",
    "        s1 = torch.sqrt(torch.sum(s1, dim=1)) # sum_i_()^{0.5}\n",
    "\n",
    "        s2 = torch.sqrt(score)\n",
    "        s2 = torch.sum(s2, dim=2)\n",
    "        s2 = s2 * s2\n",
    "        s2 = torch.sum(s2, dim=1)  # sum_time_()^{2}\n",
    "        return s1 + s2  # equation 11 in paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = nn.DataParallel(EncoderCNN())\n",
    "decoder = nn.DataParallel(DecoderRNN())\n",
    "encoder.to(device)\n",
    "decoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    encoder.module.load_state_dict(torch.load('encoder dict.pth'))\n",
    "except:\n",
    "    encoder.load_state_dict(torch.load('encoder dict.pth'))\n",
    "try:\n",
    "    decoder.module.load_state_dict(torch.load('decoder dict.pth'))\n",
    "except:\n",
    "    decoder.load_state_dict(torch.load('decoder dict.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 500\n",
    "criterion = MyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(params = decoder.parameters(), lr = 0.0001, weight_decay=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.train()\n",
    "decoder.train()\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    pbar = tqdm(total = len(dataloader),desc='Loss: -')\n",
    "    net_loss = 0\n",
    "    for idx, (img, caption, all_caption) in enumerate(dataloader):\n",
    "        img = img.to(device)\n",
    "        caption = caption.to(device) # seq_len x batch_size\n",
    "        \n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "        \n",
    "        encode_out = encoder(img)\n",
    "        try:\n",
    "            x = []\n",
    "            for out_idx in range(encode_out.shape[0]):\n",
    "                x.append([ test_dataset.vocab.stoi[a] for a in get_attr_encoded(encode_out[out_idx][None,:])])\n",
    "\n",
    "            attrs = torch.tensor(x).to(device)\n",
    "        except:\n",
    "            x = []\n",
    "            for out_idx in range(encode_out.shape[0]):\n",
    "                x.append(' '.join(get_attr_encoded(encode_out[out_idx][None,:])))\n",
    "\n",
    "            print(x)\n",
    "            print(np.vectorize(lambda x: dataset.vocab.itos[x])(capts.cpu().numpy().squeeze()))\n",
    "            raise ValueError\n",
    "            \n",
    "        hidden_logits, hidden_scores, input_scores, _ = decoder(caption.shape[1], encode_out, attrs)\n",
    "        loss = criterion(hidden_logits.transpose(1,2), caption, hidden_scores, input_scores)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        net_loss += loss.cpu().item()\n",
    "        \n",
    "        pbar.update(1)\n",
    "        pbar.set_description(f'Epoch: {epoch+1} Loss {net_loss*1.0/(idx+1):.2f}')\n",
    "        \n",
    "    pbar.refresh()\n",
    "    if (epoch+1)%20==0:\n",
    "        torch.save(encoder.state_dict(),f'encoder dict {epoch+1}.pth')\n",
    "        torch.save(decoder.state_dict(),f'decoder dict {epoch+1}.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
