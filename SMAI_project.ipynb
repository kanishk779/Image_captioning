{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "# !pip install bcolz\n",
    "import bcolz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from functools import reduce\n",
    "# from tqdm.notebook import tqdm\n",
    "import string\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import urllib\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import nltk\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as datasets \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "spacy_eng = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_img(file):\n",
    "    img_id=[]\n",
    "    for img in glob.glob(file+'*.jpg'):\n",
    "        temp=os.path.basename(img)\n",
    "        img_id.append(temp)\n",
    "        \n",
    "    return img_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the data from tar files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use glove\n",
    "glove_path = '/scratch/varunc/glove/'\n",
    "glove_zip = glove_path+'glove.6B.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_len(fname):\n",
    "    with open(fname) as f:\n",
    "        for i, l in enumerate(f):\n",
    "            pass\n",
    "    return i + 1\n",
    "\n",
    "\n",
    "# Need not run this again and again\n",
    "\n",
    "\n",
    "words = []\n",
    "idx = 0\n",
    "word2idx = {}\n",
    "vectors = bcolz.carray(np.zeros(1), rootdir=glove_path+'6B.300.dat', mode='w')\n",
    "\n",
    "with open(glove_path+'glove.6B.300d.txt', 'rb') as f:\n",
    "    pbar = tqdm(total=file_len(glove_path+'glove.6B.300d.txt'))\n",
    "    for l in f:\n",
    "        line = l.decode().split()\n",
    "        word = line[0]\n",
    "        words.append(word)\n",
    "        word2idx[word] = idx\n",
    "        idx += 1\n",
    "        vect = np.array(line[1:]).astype(np.float)\n",
    "        vectors.append(vect)\n",
    "        pbar.update(1)\n",
    "    \n",
    "vectors = bcolz.carray(vectors[1:].reshape((400000, 300)), rootdir=glove_path+'6B.300.dat', mode='w')\n",
    "vectors.flush()\n",
    "pickle.dump(words, open(glove_path+'6B.300_words.pkl', 'wb'))\n",
    "pickle.dump(word2idx, open(glove_path+'6B.300_idx.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = bcolz.open(glove_path+'6B.300.dat')[:]\n",
    "words = pickle.load(open(glove_path+'6B.300_words.pkl', 'rb'))\n",
    "word2idx = pickle.load(open(glove_path+'6B.300_idx.pkl', 'rb'))\n",
    "\n",
    "glove = {w: vectors[word2idx[w]] for w in words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_imgid=read_img(flickr_path+\"Flickr-8K/Flicker8k_Dataset/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, freq_thresh):\n",
    "        self.freq_thresh = freq_thresh\n",
    "        self.itos = {0:\"<PAD>\", 1:\"<SOS>\", 2:\"<EOS>\", 3:\"<UNK>\"}\n",
    "        self.stoi = {\"<PAD>\":0, \"<SOS>\":1, \"<EOS>\":2, \"<UNK>\":3}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenizer(text):\n",
    "        return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n",
    "\n",
    "    def build_vocab(self, sentence_list):\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        frequencies = {}\n",
    "        idx = 4\n",
    "        for sentence in sentence_list:\n",
    "            for word in self.tokenizer(sentence):\n",
    "                lw = lemmatizer.lemmatize(word)\n",
    "                if word not in frequencies:\n",
    "                    frequencies[word] = 1\n",
    "                else:\n",
    "                    frequencies[word] += 1\n",
    "                if frequencies[word] == self.freq_thresh:\n",
    "                    self.stoi[word] = idx\n",
    "                    self.itos[idx] = word\n",
    "                    idx += 1\n",
    "                    \n",
    "                if lw not in frequencies:\n",
    "                    frequencies[lw] = 1\n",
    "                else:\n",
    "                    frequencies[lw] += 1\n",
    "                if frequencies[lw] == self.freq_thresh:\n",
    "                    self.stoi[lw] = idx\n",
    "                    self.itos[idx] = lw\n",
    "                    idx += 1\n",
    "    \n",
    "    def numericalize(self, text):\n",
    "        tok_text = self.tokenizer(text)\n",
    "        vec = [self.stoi[word] if word in self.stoi else self.stoi[\"<UNK>\"] for word in tok_text]\n",
    "        return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for test and train we need to create new text files of the format (Flickr8k.token.txt)\n",
    "class FlickrDataset(Dataset):\n",
    "    def __init__(self, root_dir, caption_file, lemma_file, img_file, freq_thresh, transform=None,vocab = None, lemma_vocab=None):\n",
    "        self.freq_thresh = freq_thresh\n",
    "        self.transform = transform\n",
    "        self.root_dir = root_dir\n",
    "        self.img_file = img_file\n",
    "        self.caption_file = caption_file\n",
    "        self.lemma_file = lemma_file\n",
    "        \n",
    "        self.caption_dict = self.imgId_caption_dict()\n",
    "        self.lemma_dict = self.imgId_lemma_dict()\n",
    "        \n",
    "        self.imgs , self.captions, self.all_captions = self.load_img_caption()\n",
    "        if vocab is None:\n",
    "            self.vocab = Vocabulary(self.freq_thresh)\n",
    "            self.vocab.build_vocab(self.captions)\n",
    "        else:\n",
    "            self.vocab = vocab\n",
    "            \n",
    "        self.imgs_lemma , self.lemmas = self.load_img_lemma()\n",
    "        if lemma_vocab is None:\n",
    "            self.lemma_vocab = Vocabulary(self.freq_thresh)\n",
    "            self.lemma_vocab.build_vocab(self.lemmas)\n",
    "        else:\n",
    "            self.lemma_vocab = lemma_vocab\n",
    "\n",
    "    def imgId_caption_dict(self):\n",
    "        caption_dict = {}\n",
    "        with open(self.caption_file, 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                line = line.strip('\\n')\n",
    "                temp = line.split()\n",
    "                img_name, _ = temp[0].split('#')  # first word will be img_id\n",
    "                description = \" \".join([ word for word in temp[1:]]) # get back the description\n",
    "                if img_name not in caption_dict:\n",
    "                    caption_dict[img_name] = [description]\n",
    "                else:\n",
    "                    caption_dict[img_name].append(description)\n",
    "        return caption_dict\n",
    "\n",
    "    def imgId_lemma_dict(self):\n",
    "        stop_word=list(set(stopwords.words('english')))\n",
    "        lemma_dict = {}\n",
    "        with open(self.lemma_file, 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                line = line.strip('\\n')\n",
    "                temp = line.split()\n",
    "                img_name, _ = temp[0].split('#')  # first word will be img_id\n",
    "                description = \" \".join([ word.lower() for word in temp[1:] if word.lower() not in stop_word]) # get back the description\n",
    "                description = description.translate(str.maketrans('', '', string.punctuation)).strip()\n",
    "                if img_name not in lemma_dict:\n",
    "                    lemma_dict[img_name] = [description]\n",
    "                else:\n",
    "                    lemma_dict[img_name].append(description)\n",
    "        return lemma_dict\n",
    "\n",
    "    def load_img_caption(self):\n",
    "        imgs = []\n",
    "        captions = []\n",
    "        all_captions = []\n",
    "        with open(self.img_file, 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                line = line.strip('\\n')\n",
    "                for caption in self.caption_dict[line]:\n",
    "                    imgs.append(line)\n",
    "                    captions.append(caption)\n",
    "                    all_captions.append(self.caption_dict[line])\n",
    "        return imgs, captions, all_captions\n",
    "\n",
    "    def load_img_lemma(self):\n",
    "        imgs = []\n",
    "        captions = []\n",
    "        with open(self.img_file, 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                line = line.strip('\\n')\n",
    "                for caption in self.lemma_dict[line]:\n",
    "                    imgs.append(line)\n",
    "                    captions.append(caption)\n",
    "        return imgs, captions\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        caption = self.captions[index]\n",
    "        all_caption = self.all_captions[index]\n",
    "        img_id = self.imgs[index]\n",
    "        img = Image.open(os.path.join(self.root_dir, img_id)).convert(\"RGB\")\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        numericalized_caption = [self.vocab.stoi[\"<SOS>\"]]\n",
    "        numericalized_caption += self.vocab.numericalize(caption)\n",
    "        numericalized_caption += [self.vocab.stoi[\"<EOS>\"]]\n",
    "        \n",
    "        all_num_captions = []\n",
    "        for caption_i in all_caption:\n",
    "            num_cap = [self.vocab.stoi[\"<SOS>\"]]\n",
    "            num_cap += self.vocab.numericalize(caption_i)\n",
    "            num_cap += [self.vocab.stoi[\"<EOS>\"]]\n",
    "            all_num_captions += num_cap\n",
    "\n",
    "        return img, torch.tensor(numericalized_caption), torch.tensor(all_num_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also define simply a function instead of a class\n",
    "class MyCollate:\n",
    "    def __init__(self, pad_idx):\n",
    "        self.pad_idx = pad_idx\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        imgs = [item[0].unsqueeze(0) for item in batch]\n",
    "        imgs = torch.cat(imgs, dim=0) # batch * imgs_size * 3 (RGB)\n",
    "        captions = [item[1] for item in batch] \n",
    "        captions = pad_sequence(captions, batch_first=True, padding_value=self.pad_idx)\n",
    "        all_captions = [item[2] for item in batch]\n",
    "        all_captions = pad_sequence(all_captions, batch_first=True, padding_value=self.pad_idx)        \n",
    "        return imgs, captions, all_captions # return the batched images and captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(root_dir, caption_file, lemma_file, img_file, transform, vocab=None, lemma_vocab=None, batch=32, num_worker=2, shuffle=True, pin_memory=True):\n",
    "    dataset = FlickrDataset(root_dir=root_dir, caption_file=caption_file, lemma_file=lemma_file, img_file=img_file, freq_thresh=2, transform=transform,vocab=vocab, lemma_vocab=lemma_vocab)\n",
    "    pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n",
    "    loader = DataLoader(dataset=dataset,\n",
    "                        batch_size=batch,\n",
    "                        shuffle=shuffle,\n",
    "                        collate_fn=MyCollate(pad_idx),\n",
    "                        pin_memory=pin_memory,\n",
    "                        num_workers=num_worker)\n",
    "    return dataset,loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
